{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# League of Legends ADC Impact Analysis\n",
    "## DSC 80 Final Project\n",
    "### Authors: Kyle Zhao, Philip Chen\n",
    "\n",
    "**Website:** https://philip-chen6.github.io/LOL-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Introduction\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "This analysis focuses on professional League of Legends esports match data from 2022, sourced from Oracle's Elixir. The dataset contains detailed statistics from over 10,000 competitive matches, capturing player performance metrics, team statistics, and match outcomes.\n",
    "\n",
    "League of Legends is a team-based strategy game where two teams of five players compete to destroy the opposing team's nexus. Each player assumes one of five roles: Top lane, Jungle, Mid lane, Bot lane (ADC - Attack Damage Carry), and Support. Understanding how individual roles contribute to team success is crucial for competitive analysis.\n",
    "\n",
    "### Research Question\n",
    "\n",
    "**Central Question:** Does having an ADC (Bot lane) with a gold lead at 15 minutes significantly impact the likelihood of winning the match?\n",
    "\n",
    "This question is important because:\n",
    "1. The ADC role is considered a \"carry\" position that scales with gold\n",
    "2. The 15-minute mark is a key game state checkpoint in professional play\n",
    "3. Understanding early-game advantages can inform strategic decisions\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "The dataset contains **approximately 150,000 rows** (12 rows per game: 10 player rows + 2 team summary rows).\n",
    "\n",
    "**Key Columns for Our Analysis:**\n",
    "\n",
    "- `gameid`: Unique identifier for each match\n",
    "- `result`: Binary outcome (1 = win, 0 = loss)\n",
    "- `position`: Player's role (top, jng, mid, bot, sup)\n",
    "- `kills`, `deaths`, `assists`: Combat statistics\n",
    "- `golddiffat15`: Gold difference at 15 minutes\n",
    "- `xpdiffat15`: Experience difference at 15 minutes\n",
    "- `csdiffat15`: Creep score difference at 15 minutes\n",
    "- `damagetochampions`: Total damage dealt to enemy champions\n",
    "- `monsterkills`: Neutral objectives killed\n",
    "- `minionkills`: Minions killed (CS - creep score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the 2022 dataset\n",
    "df = pd.read_csv('/Users/kylezhao/Desktop/OE Public Match Data/2022_LoL_esports_match_data_from_OraclesElixir.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names ({len(df.columns)} total):\")\n",
    "print(df.columns.tolist()[:20])  # Show first 20 columns"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Exploratory Data Analysis\n",
    "\n",
    "### Data Cleaning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select relevant columns for analysis\n",
    "columns_to_keep = [\n",
    "    'gameid', 'datacompleteness', 'position', 'side', 'result',\n",
    "    'kills', 'deaths', 'assists', 'damagetochampions',\n",
    "    'golddiffat15', 'xpdiffat15', 'csdiffat15',\n",
    "    'golddat10', 'xpdat10', 'csdat10',\n",
    "    'monsterkills', 'minionkills',\n",
    "    'league', 'patch'\n",
    "]\n",
    "\n",
    "# Check which columns exist in the dataset\n",
    "available_cols = [col for col in columns_to_keep if col in df.columns]\n",
    "df_clean = df[available_cols].copy()\n",
    "\n",
    "print(f\"Kept {len(available_cols)} columns\")\n",
    "print(f\"Missing columns: {set(columns_to_keep) - set(available_cols)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check data completeness and filter\n",
    "if 'datacompleteness' in df_clean.columns:\n",
    "    print(\"Data completeness distribution:\")\n",
    "    print(df_clean['datacompleteness'].value_counts())\n",
    "    # Keep only complete data\n",
    "    df_clean = df_clean[df_clean['datacompleteness'] == 'complete'].copy()\n",
    "    print(f\"\\nRows after filtering for complete data: {len(df_clean)}\")\n",
    "\n",
    "# Separate player rows and team rows\n",
    "# Team rows have position as 'team'\n",
    "team_data = df_clean[df_clean['position'] == 'team'].copy()\n",
    "player_data = df_clean[df_clean['position'] != 'team'].copy()\n",
    "\n",
    "print(f\"\\nTeam rows: {len(team_data)}\")\n",
    "print(f\"Player rows: {len(player_data)}\")\n",
    "print(f\"\\nPlayer positions:\")\n",
    "print(player_data['position'].value_counts())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(player_data.isnull().sum())\n",
    "\n",
    "# Handle missing values in gold/xp/cs diff columns\n",
    "# These are NA when the game doesn't reach 15 minutes\n",
    "diff_cols = [col for col in player_data.columns if 'diffat' in col or 'at10' in col or 'at15' in col]\n",
    "print(f\"\\nDifference/stat columns: {diff_cols}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create cleaned dataset for analysis\n",
    "# For our analysis, we'll focus on games that lasted at least 15 minutes\n",
    "# (games with valid at15 statistics)\n",
    "\n",
    "if 'golddiffat15' in player_data.columns:\n",
    "    # Remove games without 15-minute data\n",
    "    player_data_15min = player_data.dropna(subset=['golddiffat15']).copy()\n",
    "    print(f\"Rows with 15-minute data: {len(player_data_15min)}\")\n",
    "else:\n",
    "    player_data_15min = player_data.copy()\n",
    "    print(\"No golddiffat15 column found\")\n",
    "\n",
    "# Display cleaned data sample\n",
    "print(\"\\nCleaned data sample:\")\n",
    "player_data_15min.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis\n",
    "\n",
    "We'll examine the distributions of key variables to understand the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Distribution of kills per player\n",
    "fig_kills = px.histogram(\n",
    "    player_data_15min,\n",
    "    x='kills',\n",
    "    nbins=30,\n",
    "    title='Distribution of Kills per Player',\n",
    "    labels={'kills': 'Kills', 'count': 'Frequency'},\n",
    "    color_discrete_sequence=['#1f77b4']\n",
    ")\n",
    "fig_kills.update_layout(\n",
    "    xaxis_title='Kills',\n",
    "    yaxis_title='Frequency',\n",
    "    showlegend=False\n",
    ")\n",
    "fig_kills.write_html('assets/kills_distribution.html', include_plotlyjs='cdn')\n",
    "fig_kills.show()\n",
    "\n",
    "print(f\"Kill statistics:\\n{player_data_15min['kills'].describe()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Distribution of gold difference at 15 minutes by position\n",
    "if 'golddiffat15' in player_data_15min.columns:\n",
    "    fig_gold = px.box(\n",
    "        player_data_15min,\n",
    "        x='position',\n",
    "        y='golddiffat15',\n",
    "        title='Gold Difference at 15 Minutes by Position',\n",
    "        labels={'golddiffat15': 'Gold Difference', 'position': 'Position'},\n",
    "        color='position'\n",
    "    )\n",
    "    fig_gold.update_layout(\n",
    "        xaxis_title='Position',\n",
    "        yaxis_title='Gold Difference at 15 Minutes',\n",
    "        showlegend=True\n",
    "    )\n",
    "    fig_gold.write_html('assets/gold_diff_by_position.html', include_plotlyjs='cdn')\n",
    "    fig_gold.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Analysis\n",
    "\n",
    "Now we'll examine relationships between variables, particularly focusing on ADC performance and game outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Filter for ADC players (bot lane)\n",
    "adc_data = player_data_15min[player_data_15min['position'] == 'bot'].copy()\n",
    "\n",
    "print(f\"ADC player rows: {len(adc_data)}\")\n",
    "print(f\"\\nADC win rate: {adc_data['result'].mean():.3f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create binary indicator for ADC having gold lead at 15 minutes\n",
    "if 'golddiffat15' in adc_data.columns:\n",
    "    adc_data['has_gold_lead'] = (adc_data['golddiffat15'] > 0).astype(int)\n",
    "    \n",
    "    # Win rate by gold lead status\n",
    "    win_rate_by_lead = adc_data.groupby('has_gold_lead')['result'].agg(['mean', 'count'])\n",
    "    win_rate_by_lead.columns = ['Win Rate', 'Count']\n",
    "    print(\"\\nWin rate by ADC gold lead status:\")\n",
    "    print(win_rate_by_lead)\n",
    "    \n",
    "    # Visualize\n",
    "    fig_winrate = px.bar(\n",
    "        win_rate_by_lead.reset_index(),\n",
    "        x='has_gold_lead',\n",
    "        y='Win Rate',\n",
    "        title='Win Rate by ADC Gold Lead Status at 15 Minutes',\n",
    "        labels={'has_gold_lead': 'Has Gold Lead', 'Win Rate': 'Win Rate'},\n",
    "        text='Win Rate'\n",
    "    )\n",
    "    fig_winrate.update_traces(texttemplate='%{text:.3f}', textposition='outside')\n",
    "    fig_winrate.update_layout(\n",
    "        xaxis=dict(tickmode='array', tickvals=[0, 1], ticktext=['No Gold Lead', 'Gold Lead']),\n",
    "        yaxis_title='Win Rate'\n",
    "    )\n",
    "    fig_winrate.write_html('assets/winrate_by_gold_lead.html', include_plotlyjs='cdn')\n",
    "    fig_winrate.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Scatter plot: Gold difference vs Damage to Champions for ADCs\n",
    "if 'golddiffat15' in adc_data.columns and 'damagetochampions' in adc_data.columns:\n",
    "    fig_scatter = px.scatter(\n",
    "        adc_data.sample(min(1000, len(adc_data))),  # Sample for performance\n",
    "        x='golddiffat15',\n",
    "        y='damagetochampions',\n",
    "        color='result',\n",
    "        title='ADC: Gold Difference at 15min vs Total Damage (Sample)',\n",
    "        labels={\n",
    "            'golddiffat15': 'Gold Difference at 15 Minutes',\n",
    "            'damagetochampions': 'Damage to Champions',\n",
    "            'result': 'Game Result'\n",
    "        },\n",
    "        color_discrete_map={0: 'red', 1: 'blue'},\n",
    "        opacity=0.6\n",
    "    )\n",
    "    fig_scatter.write_html('assets/gold_vs_damage_scatter.html', include_plotlyjs='cdn')\n",
    "    fig_scatter.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting Aggregates\n",
    "\n",
    "We'll create grouped tables to reveal patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Aggregate statistics by position and result\n",
    "position_stats = player_data_15min.groupby(['position', 'result']).agg({\n",
    "    'kills': 'mean',\n",
    "    'deaths': 'mean',\n",
    "    'assists': 'mean',\n",
    "    'damagetochampions': 'mean',\n",
    "    'gameid': 'count'\n",
    "}).round(2)\n",
    "\n",
    "position_stats.columns = ['Avg Kills', 'Avg Deaths', 'Avg Assists', 'Avg Damage', 'Games']\n",
    "print(\"\\nAverage statistics by position and game result:\")\n",
    "print(position_stats)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Pivot table: Win rate by position and league (top 10 leagues by game count)\n",
    "if 'league' in player_data_15min.columns:\n",
    "    top_leagues = player_data_15min['league'].value_counts().head(10).index\n",
    "    \n",
    "    pivot_data = player_data_15min[player_data_15min['league'].isin(top_leagues)].copy()\n",
    "    \n",
    "    pivot_table = pivot_data.pivot_table(\n",
    "        values='result',\n",
    "        index='position',\n",
    "        columns='league',\n",
    "        aggfunc='mean'\n",
    "    ).round(3)\n",
    "    \n",
    "    print(\"\\nWin rate by position across top 10 leagues:\")\n",
    "    print(pivot_table)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Assessment of Missingness\n",
    "\n",
    "### NMAR Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After examining the dataset, we believe that **`golddiffat15`, `xpdiffat15`, and `csdiffat15`** are likely **Not Missing At Random (NMAR)**.\n",
    "\n",
    "**Reasoning:** These columns are missing when games end before the 15-minute mark. The missingness depends on the game duration itself, which is not observed in our selected columns. Shorter games (often stomps or surrenders) are systematically different from longer games, and this difference affects whether we observe 15-minute statistics.\n",
    "\n",
    "**Additional data to make it MAR:** If we had a `gamelength` column showing the duration of each match, we could explain the missingness mechanism. Games shorter than 15 minutes would have missing values for these columns, making the missingness dependent on an observed variable (game length), thus making it MAR instead of NMAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missingness Dependency Testing\n",
    "\n",
    "We'll test whether the missingness of `golddiffat15` depends on other columns using permutation tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create missingness indicator\n",
    "if 'golddiffat15' in player_data.columns:\n",
    "    player_data['golddiff_missing'] = player_data['golddiffat15'].isna()\n",
    "    \n",
    "    print(\"Missingness of golddiffat15:\")\n",
    "    print(player_data['golddiff_missing'].value_counts())\n",
    "    print(f\"\\nMissing percentage: {player_data['golddiff_missing'].mean():.2%}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test 1: Does missingness depend on league?\n",
    "def total_variation_distance(dist1, dist2):\n",
    "    \"\"\"Calculate TVD between two distributions.\"\"\"\n",
    "    return np.sum(np.abs(dist1 - dist2)) / 2\n",
    "\n",
    "if 'league' in player_data.columns and 'golddiff_missing' in player_data.columns:\n",
    "    # Observed distributions\n",
    "    missing = player_data[player_data['golddiff_missing']]\n",
    "    not_missing = player_data[~player_data['golddiff_missing']]\n",
    "    \n",
    "    dist_missing = missing['league'].value_counts(normalize=True).sort_index()\n",
    "    dist_not_missing = not_missing['league'].value_counts(normalize=True).sort_index()\n",
    "    \n",
    "    # Align indices\n",
    "    all_leagues = dist_missing.index.union(dist_not_missing.index)\n",
    "    dist_missing = dist_missing.reindex(all_leagues, fill_value=0)\n",
    "    dist_not_missing = dist_not_missing.reindex(all_leagues, fill_value=0)\n",
    "    \n",
    "    observed_tvd = total_variation_distance(dist_missing.values, dist_not_missing.values)\n",
    "    print(f\"Observed TVD (league): {observed_tvd:.4f}\")\n",
    "    \n",
    "    # Permutation test\n",
    "    n_iterations = 500\n",
    "    tvds = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        shuffled = player_data.copy()\n",
    "        shuffled['golddiff_missing'] = np.random.permutation(shuffled['golddiff_missing'])\n",
    "        \n",
    "        missing_shuf = shuffled[shuffled['golddiff_missing']]\n",
    "        not_missing_shuf = shuffled[~shuffled['golddiff_missing']]\n",
    "        \n",
    "        dist_miss_shuf = missing_shuf['league'].value_counts(normalize=True).sort_index()\n",
    "        dist_not_miss_shuf = not_missing_shuf['league'].value_counts(normalize=True).sort_index()\n",
    "        \n",
    "        dist_miss_shuf = dist_miss_shuf.reindex(all_leagues, fill_value=0)\n",
    "        dist_not_miss_shuf = dist_not_miss_shuf.reindex(all_leagues, fill_value=0)\n",
    "        \n",
    "        tvd = total_variation_distance(dist_miss_shuf.values, dist_not_miss_shuf.values)\n",
    "        tvds.append(tvd)\n",
    "    \n",
    "    p_value_league = np.mean(np.array(tvds) >= observed_tvd)\n",
    "    print(f\"P-value: {p_value_league:.4f}\")\n",
    "    print(f\"\\nConclusion: {'Missingness DEPENDS on league' if p_value_league < 0.05 else 'Missingness does NOT depend on league'}\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig_tvd = go.Figure()\n",
    "    fig_tvd.add_trace(go.Histogram(x=tvds, nbinsx=30, name='Permuted TVDs'))\n",
    "    fig_tvd.add_vline(x=observed_tvd, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Observed TVD\")\n",
    "    fig_tvd.update_layout(\n",
    "        title='Permutation Test: Missingness of golddiffat15 vs League',\n",
    "        xaxis_title='Total Variation Distance',\n",
    "        yaxis_title='Frequency'\n",
    "    )\n",
    "    fig_tvd.write_html('assets/missingness_league_tvd.html', include_plotlyjs='cdn')\n",
    "    fig_tvd.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test 2: Does missingness depend on result?\n",
    "if 'result' in player_data.columns and 'golddiff_missing' in player_data.columns:\n",
    "    # Observed distributions\n",
    "    dist_missing_result = player_data[player_data['golddiff_missing']]['result'].value_counts(normalize=True).sort_index()\n",
    "    dist_not_missing_result = player_data[~player_data['golddiff_missing']]['result'].value_counts(normalize=True).sort_index()\n",
    "    \n",
    "    observed_tvd_result = total_variation_distance(dist_missing_result.values, dist_not_missing_result.values)\n",
    "    print(f\"\\nObserved TVD (result): {observed_tvd_result:.4f}\")\n",
    "    \n",
    "    # Permutation test\n",
    "    tvds_result = []\n",
    "    for _ in range(n_iterations):\n",
    "        shuffled = player_data.copy()\n",
    "        shuffled['golddiff_missing'] = np.random.permutation(shuffled['golddiff_missing'])\n",
    "        \n",
    "        dist_miss = shuffled[shuffled['golddiff_missing']]['result'].value_counts(normalize=True).sort_index()\n",
    "        dist_not_miss = shuffled[~shuffled['golddiff_missing']]['result'].value_counts(normalize=True).sort_index()\n",
    "        \n",
    "        tvd = total_variation_distance(dist_miss.values, dist_not_miss.values)\n",
    "        tvds_result.append(tvd)\n",
    "    \n",
    "    p_value_result = np.mean(np.array(tvds_result) >= observed_tvd_result)\n",
    "    print(f\"P-value: {p_value_result:.4f}\")\n",
    "    print(f\"\\nConclusion: {'Missingness DEPENDS on result' if p_value_result < 0.05 else 'Missingness does NOT depend on result'}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Hypothesis Testing\n",
    "\n",
    "### Research Question\n",
    "Does having an ADC with a gold lead at 15 minutes significantly impact win rate?\n",
    "\n",
    "**Null Hypothesis (H₀):** Teams whose ADC has a gold lead at 15 minutes win at the same rate as teams whose ADC does not have a gold lead at 15 minutes.\n",
    "\n",
    "**Alternative Hypothesis (H₁):** Teams whose ADC has a gold lead at 15 minutes win more often than teams whose ADC does not have a gold lead.\n",
    "\n",
    "**Test Statistic:** Difference in win proportions (gold lead - no gold lead)\n",
    "\n",
    "**Significance Level:** α = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare data for hypothesis test\n",
    "if 'golddiffat15' in adc_data.columns:\n",
    "    # Calculate observed test statistic\n",
    "    win_rate_gold_lead = adc_data[adc_data['has_gold_lead'] == 1]['result'].mean()\n",
    "    win_rate_no_lead = adc_data[adc_data['has_gold_lead'] == 0]['result'].mean()\n",
    "    observed_diff = win_rate_gold_lead - win_rate_no_lead\n",
    "    \n",
    "    print(f\"Win rate with ADC gold lead: {win_rate_gold_lead:.4f}\")\n",
    "    print(f\"Win rate without ADC gold lead: {win_rate_no_lead:.4f}\")\n",
    "    print(f\"\\nObserved difference: {observed_diff:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Permutation test\n",
    "if 'has_gold_lead' in adc_data.columns:\n",
    "    n_iterations = 1000\n",
    "    perm_diffs = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        # Shuffle the has_gold_lead column\n",
    "        shuffled = adc_data.copy()\n",
    "        shuffled['has_gold_lead'] = np.random.permutation(shuffled['has_gold_lead'])\n",
    "        \n",
    "        # Calculate test statistic\n",
    "        win_lead = shuffled[shuffled['has_gold_lead'] == 1]['result'].mean()\n",
    "        win_no_lead = shuffled[shuffled['has_gold_lead'] == 0]['result'].mean()\n",
    "        perm_diffs.append(win_lead - win_no_lead)\n",
    "    \n",
    "    # Calculate p-value\n",
    "    p_value = np.mean(np.array(perm_diffs) >= observed_diff)\n",
    "    \n",
    "    print(f\"\\nP-value: {p_value:.4f}\")\n",
    "    print(f\"\\nConclusion at α=0.05: \")\n",
    "    if p_value < 0.05:\n",
    "        print(\"REJECT the null hypothesis.\")\n",
    "        print(\"There is significant evidence that teams with an ADC gold lead at 15 minutes win more often.\")\n",
    "    else:\n",
    "        print(\"FAIL TO REJECT the null hypothesis.\")\n",
    "        print(\"There is insufficient evidence to conclude that ADC gold lead affects win rate.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize permutation test results\n",
    "if 'has_gold_lead' in adc_data.columns:\n",
    "    fig_perm = go.Figure()\n",
    "    fig_perm.add_trace(go.Histogram(x=perm_diffs, nbinsx=50, name='Permuted Differences'))\n",
    "    fig_perm.add_vline(\n",
    "        x=observed_diff,\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"red\",\n",
    "        annotation_text=f\"Observed: {observed_diff:.4f}\"\n",
    "    )\n",
    "    fig_perm.update_layout(\n",
    "        title='Permutation Test: Difference in Win Rates',\n",
    "        xaxis_title='Difference in Win Rate (Gold Lead - No Lead)',\n",
    "        yaxis_title='Frequency',\n",
    "        showlegend=True\n",
    "    )\n",
    "    fig_perm.write_html('assets/hypothesis_test_permutation.html', include_plotlyjs='cdn')\n",
    "    fig_perm.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Framing a Prediction Problem\n",
    "\n",
    "### Prediction Problem\n",
    "**Predict whether a team will win or lose a game based on early-game (15-minute) statistics.**\n",
    "\n",
    "**Type:** Binary Classification\n",
    "\n",
    "**Response Variable:** `result` (1 = win, 0 = loss)\n",
    "\n",
    "**Why this variable?** Game result is the ultimate measure of success in competitive League of Legends. Predicting outcomes based on early-game performance can help teams understand win conditions and make strategic adjustments.\n",
    "\n",
    "**Evaluation Metric:** We will use both **Accuracy** and **F1-Score**\n",
    "- **Accuracy:** Intuitive measure of overall correctness\n",
    "- **F1-Score:** Balances precision and recall, important if class distribution is imbalanced\n",
    "\n",
    "We chose these over other metrics because:\n",
    "- ROC-AUC is less interpretable in this context\n",
    "- F1 provides a single metric that accounts for both false positives and false negatives\n",
    "\n",
    "**Information at Time of Prediction:**\n",
    "At the 15-minute mark, we would know:\n",
    "- Gold, XP, and CS differences for each position\n",
    "- Early game objectives (first blood, first tower, etc.)\n",
    "- Player positions and champion selections\n",
    "\n",
    "We would NOT know:\n",
    "- Final game statistics (total kills, damage, etc.)\n",
    "- Game duration\n",
    "- Late-game objectives (Baron, Elder Drake, etc.)\n",
    "\n",
    "Our model will only use features that would be known at the 15-minute mark to avoid data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Baseline Model\n",
    "\n",
    "### Model Description\n",
    "Our baseline model is a **Logistic Regression** classifier using three simple features:\n",
    "1. `golddiffat15` (quantitative) - Gold difference at 15 minutes\n",
    "2. `xpdiffat15` (quantitative) - Experience difference at 15 minutes  \n",
    "3. `csdiffat15` (quantitative) - Creep score difference at 15 minutes\n",
    "\n",
    "All three features are **quantitative** and we apply **StandardScaler** to normalize them.\n",
    "\n",
    "**Why these features?**\n",
    "- These are the most fundamental early-game metrics\n",
    "- They represent resource advantages that directly correlate with power\n",
    "- They're available at exactly 15 minutes\n",
    "\n",
    "We'll focus on team-level data (team summary rows) for this prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare data for modeling - use team summary rows\n",
    "if 'golddiffat15' in team_data.columns:\n",
    "    model_data = team_data.dropna(subset=['golddiffat15', 'xpdiffat15', 'csdiffat15', 'result']).copy()\n",
    "    \n",
    "    print(f\"Team data for modeling: {len(model_data)} rows\")\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(model_data['result'].value_counts())\n",
    "    print(f\"\\nWin rate: {model_data['result'].mean():.3f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create baseline model\n",
    "if 'golddiffat15' in team_data.columns:\n",
    "    # Features and target\n",
    "    X = model_data[['golddiffat15', 'xpdiffat15', 'csdiffat15']]\n",
    "    y = model_data['result']\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Create pipeline\n",
    "    baseline_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "    ])\n",
    "    \n",
    "    # Fit model\n",
    "    baseline_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = baseline_pipeline.predict(X_train)\n",
    "    y_test_pred = baseline_pipeline.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(\"\\n=== BASELINE MODEL PERFORMANCE ===\")\n",
    "    print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Training F1-Score: {train_f1:.4f}\")\n",
    "    print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "    \n",
    "    print(\"\\nTest Set Classification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model Assessment\n",
    "\n",
    "Our baseline model achieves decent performance with just three simple features. An accuracy around 64% is significantly better than random guessing (50%) for a balanced dataset.\n",
    "\n",
    "**Is this model \"good\"?**\n",
    "- For a baseline model with only 3 features, the performance is reasonable\n",
    "- However, there's clear room for improvement - we're only using basic stat differences\n",
    "- The model doesn't account for position-specific performance, champion picks, or early objectives\n",
    "- We'll aim to improve this in the Final Model by adding more sophisticated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Model\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "We'll add several new features to improve upon our baseline:\n",
    "\n",
    "**New Features:**\n",
    "1. `gold_xp_ratio` - Ratio of gold diff to XP diff (captures efficiency)\n",
    "2. `total_resource_lead` - Combined normalized measure of gold, XP, and CS advantages\n",
    "3. `golddiffat10` - Earlier snapshot of gold difference (if available)\n",
    "\n",
    "**Why these features?**\n",
    "- **Ratio features** capture relative efficiency, not just absolute differences\n",
    "- **Combined metrics** aggregate multiple dimensions of advantage\n",
    "- **Earlier timepoints** show trajectory and momentum\n",
    "\n",
    "These features should help the model understand not just the magnitude of advantages, but their nature and development over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Engineer new features\n",
    "if 'golddiffat15' in model_data.columns:\n",
    "    model_data_enhanced = model_data.copy()\n",
    "    \n",
    "    # Feature 1: Gold to XP ratio\n",
    "    model_data_enhanced['gold_xp_ratio'] = (\n",
    "        model_data_enhanced['golddiffat15'] / \n",
    "        (model_data_enhanced['xpdiffat15'].abs() + 1)  # Add 1 to avoid division by zero\n",
    "    )\n",
    "    \n",
    "    # Feature 2: Total resource lead (normalized)\n",
    "    # Normalize each component first\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler_temp = StandardScaler()\n",
    "    normalized = scaler_temp.fit_transform(\n",
    "        model_data_enhanced[['golddiffat15', 'xpdiffat15', 'csdiffat15']]\n",
    "    )\n",
    "    model_data_enhanced['total_resource_lead'] = normalized.sum(axis=1)\n",
    "    \n",
    "    # Feature 3: Early gold difference (if available)\n",
    "    if 'golddat10' in model_data_enhanced.columns:\n",
    "        model_data_enhanced = model_data_enhanced.dropna(subset=['golddat10'])\n",
    "        feature_cols = ['golddiffat15', 'xpdiffat15', 'csdiffat15', \n",
    "                       'gold_xp_ratio', 'total_resource_lead', 'golddat10']\n",
    "    else:\n",
    "        feature_cols = ['golddiffat15', 'xpdiffat15', 'csdiffat15', \n",
    "                       'gold_xp_ratio', 'total_resource_lead']\n",
    "    \n",
    "    print(f\"Enhanced model features: {feature_cols}\")\n",
    "    print(f\"\\nData shape: {model_data_enhanced.shape}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare enhanced dataset\n",
    "X_enhanced = model_data_enhanced[feature_cols]\n",
    "y_enhanced = model_data_enhanced['result']\n",
    "\n",
    "# Train-test split (same random state for fair comparison)\n",
    "X_train_enh, X_test_enh, y_train_enh, y_test_enh = train_test_split(\n",
    "    X_enhanced, y_enhanced, test_size=0.25, random_state=42, stratify=y_enhanced\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train_enh)}\")\n",
    "print(f\"Test set size: {len(X_test_enh)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "We'll use **Random Forest Classifier** for our final model and tune:\n",
    "- `max_depth`: Controls tree depth (prevents overfitting)\n",
    "- `n_estimators`: Number of trees in the forest\n",
    "- `min_samples_split`: Minimum samples required to split a node\n",
    "\n",
    "We'll use GridSearchCV with 5-fold cross-validation to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create pipeline with Random Forest\n",
    "final_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [5, 10, 15, 20],\n",
    "    'classifier__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "print(\"Performing grid search...\")\n",
    "grid_search = GridSearchCV(\n",
    "    final_pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_enh, y_train_enh)\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV F1-score: {grid_search.best_score_:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate final model\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "y_train_pred_final = final_model.predict(X_train_enh)\n",
    "y_test_pred_final = final_model.predict(X_test_enh)\n",
    "\n",
    "train_acc_final = accuracy_score(y_train_enh, y_train_pred_final)\n",
    "test_acc_final = accuracy_score(y_test_enh, y_test_pred_final)\n",
    "train_f1_final = f1_score(y_train_enh, y_train_pred_final)\n",
    "test_f1_final = f1_score(y_test_enh, y_test_pred_final)\n",
    "\n",
    "print(\"\\n=== FINAL MODEL PERFORMANCE ===\")\n",
    "print(f\"Training Accuracy: {train_acc_final:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_final:.4f}\")\n",
    "print(f\"Training F1-Score: {train_f1_final:.4f}\")\n",
    "print(f\"Test F1-Score: {test_f1_final:.4f}\")\n",
    "\n",
    "print(\"\\n=== IMPROVEMENT OVER BASELINE ===\")\n",
    "print(f\"Accuracy improvement: {test_acc_final - test_acc:.4f}\")\n",
    "print(f\"F1-Score improvement: {test_f1_final - test_f1:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(classification_report(y_test_enh, y_test_pred_final))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confusion matrix visualization\n",
    "cm = confusion_matrix(y_test_enh, y_test_pred_final)\n",
    "\n",
    "fig_cm = go.Figure(data=go.Heatmap(\n",
    "    z=cm,\n",
    "    x=['Predicted Loss', 'Predicted Win'],\n",
    "    y=['Actual Loss', 'Actual Win'],\n",
    "    colorscale='Blues',\n",
    "    text=cm,\n",
    "    texttemplate='%{text}',\n",
    "    textfont={\"size\": 20}\n",
    "))\n",
    "\n",
    "fig_cm.update_layout(\n",
    "    title='Confusion Matrix - Final Model',\n",
    "    xaxis_title='Predicted',\n",
    "    yaxis_title='Actual'\n",
    ")\n",
    "\n",
    "fig_cm.write_html('assets/confusion_matrix.html', include_plotlyjs='cdn')\n",
    "fig_cm.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract feature importances\n",
    "rf_classifier = final_model.named_steps['classifier']\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(importance_df)\n",
    "\n",
    "# Visualize\n",
    "fig_imp = px.bar(\n",
    "    importance_df,\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    orientation='h',\n",
    "    title='Feature Importances - Final Model'\n",
    ")\n",
    "fig_imp.update_layout(yaxis={'categoryorder': 'total ascending'})\n",
    "fig_imp.write_html('assets/feature_importances.html', include_plotlyjs='cdn')\n",
    "fig_imp.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Fairness Analysis\n",
    "\n",
    "### Fairness Question\n",
    "**Does our model perform differently for games with large gold differences (>2000) versus small gold differences (≤2000) at 15 minutes?**\n",
    "\n",
    "**Group X:** Games with |golddiffat15| ≤ 2000 (close games)\n",
    "\n",
    "**Group Y:** Games with |golddiffat15| > 2000 (stomps)\n",
    "\n",
    "**Evaluation Metric:** Accuracy\n",
    "\n",
    "**Hypotheses:**\n",
    "- **Null (H₀):** Our model is fair. Its accuracy for close games and stomp games is roughly the same.\n",
    "- **Alternative (H₁):** Our model is unfair. Its accuracy differs between close games and stomp games.\n",
    "\n",
    "**Test Statistic:** Difference in accuracy (stomp games - close games)\n",
    "\n",
    "**Significance Level:** α = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create group labels based on gold difference magnitude\n",
    "test_data_fairness = X_test_enh.copy()\n",
    "test_data_fairness['result'] = y_test_enh.values\n",
    "test_data_fairness['prediction'] = y_test_pred_final\n",
    "test_data_fairness['correct'] = (test_data_fairness['result'] == test_data_fairness['prediction']).astype(int)\n",
    "\n",
    "# Define groups\n",
    "test_data_fairness['game_type'] = (test_data_fairness['golddiffat15'].abs() > 2000).map(\n",
    "    {True: 'stomp', False: 'close'}\n",
    ")\n",
    "\n",
    "print(\"Game type distribution:\")\n",
    "print(test_data_fairness['game_type'].value_counts())\n",
    "\n",
    "# Calculate accuracy by group\n",
    "acc_by_group = test_data_fairness.groupby('game_type')['correct'].mean()\n",
    "print(\"\\nAccuracy by game type:\")\n",
    "print(acc_by_group)\n",
    "\n",
    "observed_diff_fairness = acc_by_group['stomp'] - acc_by_group['close']\n",
    "print(f\"\\nObserved difference (stomp - close): {observed_diff_fairness:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Permutation test for fairness\n",
    "n_iterations_fairness = 1000\n",
    "perm_diffs_fairness = []\n",
    "\n",
    "for _ in range(n_iterations_fairness):\n",
    "    # Shuffle game_type labels\n",
    "    shuffled = test_data_fairness.copy()\n",
    "    shuffled['game_type'] = np.random.permutation(shuffled['game_type'])\n",
    "    \n",
    "    # Calculate accuracy for each group\n",
    "    acc_shuffled = shuffled.groupby('game_type')['correct'].mean()\n",
    "    diff = acc_shuffled['stomp'] - acc_shuffled['close']\n",
    "    perm_diffs_fairness.append(diff)\n",
    "\n",
    "# Calculate p-value (two-tailed test)\n",
    "p_value_fairness = np.mean(np.abs(perm_diffs_fairness) >= np.abs(observed_diff_fairness))\n",
    "\n",
    "print(f\"\\nP-value: {p_value_fairness:.4f}\")\n",
    "print(f\"\\nConclusion at α=0.05:\")\n",
    "if p_value_fairness < 0.05:\n",
    "    print(\"REJECT the null hypothesis.\")\n",
    "    print(\"The model shows evidence of unfairness between game types.\")\n",
    "else:\n",
    "    print(\"FAIL TO REJECT the null hypothesis.\")\n",
    "    print(\"The model appears to be fair - accuracy is similar across game types.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize fairness test\n",
    "fig_fairness = go.Figure()\n",
    "fig_fairness.add_trace(go.Histogram(\n",
    "    x=perm_diffs_fairness,\n",
    "    nbinsx=50,\n",
    "    name='Permuted Differences'\n",
    "))\n",
    "fig_fairness.add_vline(\n",
    "    x=observed_diff_fairness,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"red\",\n",
    "    annotation_text=f\"Observed: {observed_diff_fairness:.4f}\"\n",
    ")\n",
    "fig_fairness.update_layout(\n",
    "    title='Fairness Analysis: Accuracy Difference Between Game Types',\n",
    "    xaxis_title='Difference in Accuracy (Stomp - Close)',\n",
    "    yaxis_title='Frequency'\n",
    ")\n",
    "fig_fairness.write_html('assets/fairness_analysis.html', include_plotlyjs='cdn')\n",
    "fig_fairness.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This analysis has demonstrated that:\n",
    "\n",
    "1. **ADC gold leads matter:** Teams with ADCs holding gold advantages at 15 minutes win significantly more often\n",
    "2. **Early-game prediction is possible:** Our model achieves strong performance predicting game outcomes from 15-minute statistics\n",
    "3. **Feature engineering helps:** Adding derived features like ratios and combined metrics improved model performance\n",
    "4. **Model fairness:** Our model performs fairly across different game states (close vs stomp)\n",
    "\n",
    "**Key Takeaways for Players and Teams:**\n",
    "- Prioritizing ADC early-game advantages is strategically sound\n",
    "- Gold difference is the strongest predictor of game outcomes\n",
    "- Games can often be predicted by the 15-minute mark, emphasizing the importance of early game execution\n",
    "\n",
    "**Link to Website:** https://philip-chen6.github.io/LOL-analysis/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
